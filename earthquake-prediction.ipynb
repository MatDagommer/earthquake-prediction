{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqdSSxEJLtds"
   },
   "source": [
    "# *KAGGLE CHALLENGE: LANL Earthquake Prediction*\n",
    "\n",
    "Un projet de Matthieu Dagommer, Paul Boulgakoff, Godefroy Bichon, Germain L'Hostis\n",
    "\n",
    "Versions utilisées:\n",
    "\n",
    "Python: 3.10.4\n",
    "Torch: 1.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import torch as th\n",
    "th.cuda.empty_cache()\n",
    "\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "#from torchviz import make_dot\n",
    "\n",
    "from torchsummary import summary \n",
    "\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import kurtosis, skew\n",
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "seed = 1\n",
    "th.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "GeForce GTX 1050 Ti with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "### Ensuring CPU is connected\n",
    "\n",
    "print(th.cuda.is_available())\n",
    "print(th.cuda.get_device_name())\n",
    "device = th.device('cuda' if th.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *General Hyperparameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting General Hyperparameters \n",
    "\n",
    "size_batch = 100 # nombres de patchs par batchs\n",
    "valid_rate = 0.1 # proportion des données disponibles consacrée à la validation\n",
    "overlap_rate = 0.2 # taux de recouvrement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Data Preparation*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading Data\n",
    "\n",
    "rootpath = os.getcwd() + \"/\"\n",
    "nrows = 200000000\n",
    "\n",
    "# Possible de tout charger dans la RAM comme ceci (prend 9 GB environ si on charge les 629 145 480 lignes)\n",
    "train_data = pd.read_csv(rootpath + \"train.csv\", usecols = ['acoustic_data', 'time_to_failure'], \\\n",
    "                         dtype = {'acoustic_data': np.int16, 'time_to_failure': np.float64}, nrows = nrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = th.squeeze(th.tensor(train_data['acoustic_data'].values, dtype = th.int16))\n",
    "Y = th.squeeze(th.tensor(train_data['time_to_failure'].values, dtype = th.float64))\n",
    "\n",
    "del train_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tvNnBnxCsLNS"
   },
   "outputs": [],
   "source": [
    "### Retrieve Test Data\n",
    "\n",
    "path = rootpath + \"test/\" + \"seg_00030f.csv\"\n",
    "test = pd.read_csv(path)\n",
    "time_window = test.shape[0]\n",
    "\n",
    "# Les lignes ci-dessous prennent du temps à calculer et servent à charger les données test,\n",
    "# dont on a pas besoin pour le moment.\n",
    "# elles ont été enlevées temporairement pour lancer le code sous drive:\n",
    "\n",
    "#path = rootpath + \"sample_submission.csv\"\n",
    "#sample_submission = pd.read_csv(path)\n",
    "\n",
    "\n",
    "#checking the ttf of test samples (ALL ZEROS ?!?)\n",
    "#test_ttf_counts = sample_submission.time_to_failure.value_counts()\n",
    "\n",
    "\n",
    "#Importation des données à prédire\n",
    "#X_test = []\n",
    "\n",
    "#for filename in os.listdir(rootpath + \"test\"):\n",
    "#    temp_df = pd.read_csv(rootpath + \"test/\" + filename)\n",
    "#    X_test.append(temp_df)\n",
    "\n",
    "#Xtest = th.zeros((len(X_test),time_window))\n",
    "#for i in range(len(X_test)):\n",
    "#  Xtest[i,:] = th.tensor(X_test[i][\"acoustic_data\"], dtype = th.float32)\n",
    "\n",
    "#Ytest = th.tensor(sample_submission[\"time_to_failure\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ORUQsR8PDg2K"
   },
   "outputs": [],
   "source": [
    "size_patch = time_window # taille des patchs calquée sur la longueur des séries test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nn_5lrssRSpJ",
    "outputId": "e0eb114e-3c57-458a-bec2-53709bbd85d8"
   },
   "outputs": [],
   "source": [
    "### Function to create patched sequences with some overlap out of the training data\n",
    "\n",
    "# Le recouvrement est exprimé en proportion de la taille d'une fenêtre (time_window)\n",
    "def patching(size_patch, X, Y, overlap_rate):\n",
    "    \n",
    "    overlap = int(size_patch*overlap_rate)\n",
    "    L = X.shape[0] # total length of the training acoustic signal\n",
    "    n_patch = int(np.floor((L-size_patch)/(size_patch-overlap)+1))\n",
    "    ids_no_seism = []\n",
    "\n",
    "    X_patch = th.zeros(n_patch,size_patch)\n",
    "    Y_patch = th.zeros(n_patch)\n",
    "    \n",
    "    for i in range (n_patch):\n",
    "        X_patch[i,:] = X[i*(size_patch-overlap):(i+1)*size_patch-i*overlap] \n",
    "        Y_patch[i] = Y[(i+1)*size_patch - i*overlap] \n",
    "    \n",
    "        # Relever les patchs qui ne contiennent pas de séisme\n",
    "        if th.min(Y[i*(size_patch-overlap):(i+1)*size_patch - i*overlap]) > 0.001:\n",
    "            ids_no_seism.append(i)\n",
    "    \n",
    "    X_patch = X_patch[ids_no_seism]\n",
    "    Y_patch = Y_patch[ids_no_seism]\n",
    "    \n",
    "    return(n_patch, X_patch, Y_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normalizing Data\n",
    "\n",
    "ss = StandardScaler()\n",
    "mm = MinMaxScaler()\n",
    "    \n",
    "X = th.squeeze(th.from_numpy(ss.fit_transform(np.array(X).reshape(-1, 1))))\n",
    "Y = th.squeeze(th.from_numpy(mm.fit_transform(np.array(Y).reshape(-1, 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1656, 150000])\n",
      "There are  1656 time series available for training and validation after patching with overlap. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Distribution of data in patches\n",
    "\n",
    "_, X_patch, Y_patch = patching(size_patch, X, Y, overlap_rate = overlap_rate)\n",
    "\n",
    "del X; del Y\n",
    "gc.collect()\n",
    "\n",
    "print(X_patch.shape)\n",
    "print(\"There are \", X_patch.shape[0], \"time series available for training and validation after patching with overlap. \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initializating Data Sets\n",
    "\n",
    "\n",
    "# shuffling data\n",
    "idx = np.arange(X_patch.shape[0]) # indices\n",
    "shuffled_idx = np.random.shuffle(idx)\n",
    "X_patch = th.squeeze(X_patch[shuffled_idx,:])\n",
    "Y_patch = th.squeeze(Y_patch[shuffled_idx])\n",
    "\n",
    "N_samples = X_patch.shape[0]\n",
    "N_valid = int(valid_rate*N_samples) # number of validation patches\n",
    "\n",
    "X_valid = X_patch[:N_valid,:].cpu()\n",
    "Y_valid = Y_patch[:N_valid].cpu()\n",
    "\n",
    "# Les inputs de nn.Conv1d doivent être de la forme (N, C_in, *),\n",
    "#avec N: nombre de samples/taille batch, C_in: le nombre de channels et * les dimensions des objets (nos séries)\n",
    "# on rajoute la dimension channel (1):\n",
    "X_valid = th.unsqueeze(X_valid, 1)\n",
    "\n",
    "#On fait la même chose pour les données X_train, mais directement au moment de constituer les batchs dans la for-loop !\n",
    "X_train = X_patch[N_valid:,:].cpu()\n",
    "Y_train = Y_patch[N_valid:].cpu()\n",
    "N_train = X_train.shape[0]\n",
    "\n",
    "n_batch = int(X_train.shape[0] / size_batch) # nombre total de batchs\n",
    "\n",
    "X_train = X_train.reshape([N_train, 1, size_patch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Function for Results plotting*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotting Graphs\n",
    "\n",
    "def plot_and_save_results(train_losses, valid_losses, best_mvd, best_mtd, mm, model_name):\n",
    "\n",
    "    best_epoch = np.fromiter(valid_losses, dtype=float).argmin()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize = (20,10))\n",
    "    plt.rcParams['font.size'] = '20'\n",
    "    ax[0].set(title = \"Losses: Training and Validation\") \n",
    "    ax[0].set_xlabel(\"epochs\", fontsize = 20)\n",
    "    ax[0].set_ylabel(\"MSE\", fontsize = 20)\n",
    "    ax[0].plot(train_losses,\"r\", label = \"Training\", linewidth = 3)\n",
    "    ax[0].plot(valid_losses, \"b\", label = \"Validation\", linewidth = 3)\n",
    "    ax[0].legend(loc = \"upper right\", fontsize = 18)\n",
    "    ax[0].axvline(x=int(best_epoch), color = 'black', linestyle =\"--\", linewidth = 3)\n",
    "    ax[0].annotate(\"Best epoch: {}\\nMSE_train: {:.3f}\\nMSE_valid: {:.3f}\".format(int(best_epoch), min_tl, min_vl), \\\n",
    "                   xy = (0.5,0.5), xycoords = 'axes fraction')\n",
    "    \n",
    "    best_mvd_plot = mm.inverse_transform(best_mvd.reshape(-1, 1))\n",
    "    best_mtd_plot = mm.inverse_transform(best_mtd.reshape(-1, 1))\n",
    "\n",
    "    N_valid = best_mvd_plot.shape[0]\n",
    "    \n",
    "    mean = float(np.mean(best_mvd_plot))\n",
    "    std_dev = float(np.std(best_mvd_plot))\n",
    "    kurt = float(kurtosis(best_mvd_plot))\n",
    "    skewn = float(skew(best_mvd_plot))\n",
    "    q1 = float(np.quantile(best_mvd_plot, 0.25))\n",
    "    median = float(np.quantile(best_mvd_plot, 0.5))\n",
    "    q3 = float(np.quantile(best_mvd_plot, 0.75))\n",
    "    mae = np.absolute(best_mvd_plot).sum() / N_valid\n",
    "    mse = np.square(best_mvd_plot).sum() / N_valid\n",
    "    \n",
    "    text = \"mean: {:.3f}\\nstd: {:.3f}\\nkurt: {:.3f}\\nskew: {:.3f}\\nq1: {:.3f}\\nmed: {:.3f}\\nq3: {:.3f}\\niqr: {:.3f}\\nmae: {:.3f}\\nmse: {:.3f}\\n\".format(mean, std_dev, kurt, skewn, q1, median, q3, q3-q1, mae, mse)\n",
    "    \n",
    "    ax[1].hist(best_mvd_plot, alpha = 0.3, label = \"validation set\", bins = 100, density = True, range = (-16, 16))\n",
    "    ax[1].set(title = \"TTF error distributions at best epoch\")\n",
    "    ax[1].set_xlabel(\"Error (seconds)\", fontsize = 20)\n",
    "    ax[1].set_ylabel(\"Density\", fontsize = 20)\n",
    "    ax[1].hist(best_mtd_plot, alpha = 0.3, label = \"training set\", bins = 100, density = True, range = (-16, 16))\n",
    "    ax[1].annotate(text, xy =(-15, 0.1))\n",
    "    ax[1].legend()\n",
    "\n",
    "\n",
    "    plt.gcf()\n",
    "    plt.savefig(model_name + \"_plot.jpg\")\n",
    "    plt.show()\n",
    "\n",
    "    model_features = {\"N_samples\": N_samples, \"N_train\": N_train, \"N_valid\": N_valid, \\\n",
    "                      \"overlap_rate\": overlap_rate, \"learning_rate\": learning_rate, \"num_epochs\": num_epochs, \\\n",
    "                     \"seed\": seed, \"size_batch\": size_batch, \"train_losses\": train_losses, \"valid_losses\": valid_losses, \\\n",
    "                     \"valid_differences\": valid_differences, \"best_mtd\": best_mtd, \"best_mvd\": best_mvd, \"min_tl\": min_tl, \\\n",
    "                      \"min_vl\": min_vl}\n",
    "\n",
    "    pickle.dump(model_features, open(model_name + \".p\", \"wb\" ))\n",
    "    \n",
    "    model_features_display = {\"N_samples\": N_samples, \"N_train\": N_train, \"N_valid\": N_valid, \\\n",
    "                  \"overlap_rate\": overlap_rate, \"learning_rate\": learning_rate, \"num_epochs\": num_epochs, \\\n",
    "                 \"seed\": seed, \"size_batch\": size_batch}\n",
    "    \n",
    "    pickle.dump(model_features_display, open(model_name + \"display.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *1D Convolutional Neural Network*\n",
    "\n",
    "Cette partie contient les architectures de deux modèles de convolution 1D qui ont été testés au cours du projet, ainsi que la définition des hyperparamètres et la boucle d'entraînement associée.\n",
    "\n",
    "Pour lancer les calculs avec le LSTM, vous pouvez sauter ces cellules et vous rendre à la partie concernée située plus bas !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters specific to 1D CNN\n",
    "\n",
    "learning_rate=0.0001\n",
    "num_epochs=500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOtetCjsDj8t"
   },
   "outputs": [],
   "source": [
    "### 1D Convolutional Network #1\n",
    "\n",
    "class NN(nn.Module):\n",
    "    \n",
    "    def __init__ (self,num_classes=1):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1=nn.Conv1d(in_channels=1, out_channels=1, \n",
    "                             kernel_size=10, padding=1, stride=5)\n",
    "        self.fc2=nn.AdaptiveMaxPool1d(200)\n",
    "        self.fct3=nn.Linear(200,50)\n",
    "        self.fct4=nn.Linear(50,num_classes)\n",
    "    \n",
    "    def forward (self,x):\n",
    "        x=self.fc1(x)\n",
    "        x=self.fc2(x)\n",
    "        x=self.fct3(x)\n",
    "        x=self.fct4(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1D Convolutional Network #2\n",
    "\n",
    "class NN_2(nn.Module):\n",
    "    def __init__(self,num_classes=1):\n",
    "        super(NN_2, self).__init__()\n",
    "        self.seq_1=nn.Sequential(nn.Conv1d(in_channels = 1, out_channels = 16, kernel_size = 10, stride = 5),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool1d(kernel_size = 2),\n",
    "                                 nn.Conv1d(in_channels = 16, out_channels = 32, kernel_size = 10, stride = 5, \\\n",
    "                                           padding = 'valid'),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool1d(kernel_size = 2),\n",
    "                                 nn.Conv1d(in_channels = 32, out_channels = 64, kernel_size = 10, stride = 5, \\\n",
    "                                           padding = 'valid'),\n",
    "                                 nn.ReLU()\n",
    "                                )\n",
    "        \n",
    "        self.seq_2=nn.Sequential(nn.Dropout(p = 0.4),\n",
    "                                 nn.Linear(in_features = 64, out_features = 1)\n",
    "                                )\n",
    "    def forward(self,x):\n",
    "        x=self.seq_1(x)\n",
    "        x = th.mean(x, dim = 2, keepdim = True)\n",
    "        x = th.squeeze(x)\n",
    "        x=self.seq_2(x)\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hH24ktcBDvrq",
    "outputId": "55f4b5f1-f013-4e34-da59-49e8d0d19e99"
   },
   "outputs": [],
   "source": [
    "### Initialize Network, Define Loss and optimizer\n",
    "\n",
    "model = NN_2()\n",
    "print(model)\n",
    "model.to(device)\n",
    "#summary(model, (1, time_window))\n",
    "loss=nn.MSELoss()\n",
    "optimizer=th.optim.Adam(model.parameters(),learning_rate)\n",
    "\n",
    "#yhat = model(X_valid.to(device))\n",
    "#make_dot(yhat, params=dict(list(model.named_parameters()))).render(\"cnn_2\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jIl7K2gvZd3U"
   },
   "outputs": [],
   "source": [
    "### Check accuracy during training\n",
    "\n",
    "def check_accuracy(X_valid, Y_valid, model):\n",
    "    \n",
    "    device = 'cuda'\n",
    "    N_valid = X_valid.shape[0]\n",
    "    real_ttf = th.squeeze(Y_valid)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    with th.no_grad():\n",
    "\n",
    "        scores = th.squeeze(model(X_valid.to(device))).cpu()\n",
    "        valid_loss_ = loss_fn(scores, real_ttf).cpu()\n",
    "        valid_differences = scores[:] - real_ttf[:]\n",
    "        valid_differences = valid_differences.cpu().numpy()\n",
    "        valid_loss = valid_loss_.item()\n",
    "    \n",
    "    return valid_loss, valid_differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QVS0pEmAD4P4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Train network\n",
    "\n",
    "import time\n",
    "th.cuda.empty_cache()\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "best_mvd = [] # valid differences at best epoch\n",
    "best_mtd = [] # training differences at best epoch\n",
    "min_vl = 1000\n",
    "\n",
    "N_train_per_epoch = n_batch*size_batch \n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    th.cuda.empty_cache()\n",
    "    \n",
    "    # A chaque époque, on constitue aléatoirement de nouveaux batchs\n",
    "    random_idx = np.random.randint(0, N_train, N_train_per_epoch)\n",
    "    X_train_batch = th.reshape(X_train[random_idx], [n_batch, size_batch, 1, size_patch])\n",
    "    Y_train_batch = th.reshape(Y_train[random_idx], [n_batch, size_batch, 1, 1])\n",
    "    \n",
    "    running_loss = 0\n",
    "    _loss = 0\n",
    "\n",
    "    for ids in range (n_batch):\n",
    "        \n",
    "        #On met les gradients à zéro \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Prédiction du modèle\n",
    "        predicted_ttfs = th.squeeze(model(X_train_batch[ids].to(device))).cpu()\n",
    "        # Calcul de la fonction de perte\n",
    "        _loss = loss(predicted_ttfs, th.squeeze(Y_train_batch[ids]))\n",
    "        \n",
    "        del predicted_ttfs\n",
    "        gc.collect()\n",
    "        \n",
    "        # Backpropagation\n",
    "        _loss.backward()\n",
    "        running_loss += _loss.item()\n",
    "        \n",
    "        # Mise à jour des paramètres du modèle\n",
    "        optimizer.step()\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    th.cuda.empty_cache()\n",
    "    \n",
    "    valid_loss, valid_differences = check_accuracy(X_valid, Y_valid, model)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_losses.append(running_loss / n_batch)\n",
    "    \n",
    "    if valid_loss < min_vl:\n",
    "        \n",
    "        min_vl = valid_loss\n",
    "        min_tl = running_loss\n",
    "        \n",
    "        best_mvd = valid_differences\n",
    "        \n",
    "        Y_final = th.squeeze(model(X_train[:int(N_train/10)].to(device))).cpu()\n",
    "        \n",
    "        best_mtd = Y_train[:int(N_train/10)] - Y_final[:]\n",
    "        best_mtd = best_mtd.cpu().detach().numpy()\n",
    "        \n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        print(\"Epoch: {}\\t\".format(epoch),\n",
    "                \"train Loss: {:.5f}.. \".format(train_losses[-1]),\n",
    "                \"valid Loss: {:.5f}.. \".format(valid_losses[-1])) \n",
    "\n",
    "print(\"---------- Best : {:.3f}\".format(min(valid_losses)), \" at epoch \" \n",
    "    , np.fromiter(valid_losses, dtype=float).argmin(), \" / \", epoch + 1)\n",
    "\n",
    "\n",
    "end = time.perf_counter()\n",
    "print(\"\\ntime elapsed: {:.3f}\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = input(\"Choose a name for the model: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_and_save_results(train_losses, valid_losses, best_mvd, best_mtd, mm, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"training_003\"\n",
    "model_features = pickle.load(open(model_name + \".p\", \"rb\" ))\n",
    "print(model_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_train.shape)\n",
    "Y_train_display = np.array(Y_train)\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(Y_train_display, bins=50)\n",
    "mean_Y = np.mean(Y_train_display)\n",
    "ax.axvline(mean_Y, ymin=0, ymax=1, color = 'red', linewidth = 5)\n",
    "ax.text(7, 10, \"mean = {:.3f} s\".format(mean_Y), color = \"red\")\n",
    "ax.set(xlabel = \"TTF (s)\", ylabel = \"Occurences\", title=\"TTF distribution (train set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cs_ETl-ID_9U"
   },
   "source": [
    "# *Quelques commentaires sur les modifications:*\n",
    "\n",
    "**Attention, je fais une distinction \"patch\" et \"batch\" !**\n",
    "\n",
    "Nos données d'entrées sont des séries temporelles. Je les ai découpées en \"**patchs**\", c'est-à-dire en échantillons de même longueur. \n",
    "\n",
    "J'ai choisi cette longueur de sorte que les données d'entraînement soient au même format que les données test (On a des séries test d'une longueur de 150000, j'ai donc découpé en bouts de 150000 les données d'entraînement) : autrement le modèle risque de ne pas être approprié pour réaliser la tâche de prédiction à laquelle on le destine. J'ai gardé l'idée du recouvrement, qui était top.\n",
    "\n",
    "A partir de là, un échantillon ou point de données correspond à un couple (X, Y) où X est la série temporelle (patch, acoustic_data) et Y le temps avant séisme à la fin du patch (time_to_failure).\n",
    "\n",
    "On peut ensuite regrouper plusieurs points (X,Y) dans des **\"batchs\"**. Le regroupement en batch est juste une technique qui sert à fluidifier la descente de gradient: on pourrait en théorie \"balancer tout le jeu d'entraînement\" au modèle (dans ce cas on dit qu'on fait une \"Batch Gradient Descent\") mais il est trop gros en pratique et demanderait trop de mémoire. On fait donc une \"Mini-Batch Gradient Descent\", on \"nourrit\" le modèle avec des petits paquets de données au fur et à mesure.\n",
    "\n",
    "A chaque époque, le modèle voit passer l'intégralité du jeu de données.\n",
    "\n",
    "Cet article résume bien les différentes façons d'aborder la descente de gradient :\n",
    "https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a\n",
    "\n",
    "\n",
    "**Les données de validation doivent être différentes des données d'entraînement**\n",
    "\n",
    "J'ai créé un jeu de données de validation à part du jeu d'entraînement.\n",
    "L'intérêt du jeu de validation est de voir comment le modèle qu'on a entraîné avec les données d'entraînement se démerde avec des données qu'il n'a jamais vues. Au final c'est l'erreur faite sur le jeu de validation qui nous importe: plus cette erreur est faible, plus le modèle fait de bonnes prédictions. \n",
    "\n",
    "**En résumé :**\n",
    "\n",
    "Je charge les données d'entraînement dans X et Y, puis je les découpe avec recouvrement avec la fonction patching() dans X_patch et Y_patch. \n",
    "A partir de là, je sépare les données d'entraînement (X_train et Y_train) des données de validation (X_valid et Y_valid).\n",
    "Au moment d'entraîner le modèle, je répartis aléatoirement X_train et Y_train dans des batchs différents à chaque époque (X_train_batch et Y_train_batch).\n",
    "\n",
    "**Ce qu'il reste à faire:**\n",
    "- Essayer d'exécuter le code avec l'intégralité des données d'entraînement et de test. Pour ça, trouver un moyen de charger les données d'entraînement ultra-volumineuses.\n",
    "- Retrouver le taux d'acquisition du signal pour pouvoir exprimer le time_to_failure avec une unité de temps. \n",
    "- Faire un plan d'expérience: il faut essayer plusieurs modèles/architectures, et optimiser les hyperparamètres (learning_rate, nombre de couches cachées) pour chaque modèle. Pour savoir si on est bons, ce serait bien de regarder ce que les gens ont réussi à faire de mieux en termes de prédiction sur kaggle.\n",
    "- Eventuellement ajouter de l'apprentissage avec Dropout, ajouter de la BatchNorm entre les couches cachées (il me semble que c'est fait automatiquement, mais ça vaut le coup de vérifier)\n",
    "- Comparer les performances des modèles entre eux avec les données test\n",
    "- Pour l'instant on a opté pour une méthode de \"bourrins\" (aucun prétraitement des données) mais cet article https://www.kaggle.com/code/allunia/shaking-earth/notebook#Let's-get-familiar-with-the-data propose des pistes intéressantes (moyennage des séries avec une fenêtre roulante, extraction de paramètres statistiques de la série). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqi1BoEdwan0"
   },
   "source": [
    "# *LSTM*\n",
    "\n",
    "Le LSTM est une architecture adaptée pour les données en séries temporelles.\n",
    "C'est une version \"améliorée\" d'un réseau de neurones récurrent (RNN).\n",
    "\n",
    "Note du site de Pytorch:\n",
    "\n",
    "\"Before getting to the example, note a few things. Pytorch’s LSTM expects all of its inputs to be 3D tensors. The semantics of the axes of these tensors is important. The first axis is the sequence itself, the second indexes instances in the mini-batch, and the third indexes elements of the input.\"\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters specific to LSTM\n",
    "\n",
    "num_epochs = 1000 #1000 epochs\n",
    "learning_rate = 0.001 #0.001 lr\n",
    "hidden_size = 1 #number of features in hidden state\n",
    "num_layers = 1 #number of stacked lstm layers => should stay at 1 unless you want to combine two LSTMs together\n",
    "N_sub_patches = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "rEsaMroYSTf_",
    "outputId": "ee282ea0-9e14-4ad5-bd2a-52b7795bc830"
   },
   "outputs": [],
   "source": [
    "N_features = 10 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_4ULTZ0TYn8A"
   },
   "outputs": [],
   "source": [
    "def data_preparation_2(X_train, Y_train, X_valid, Y_valid, size_patch, N_sub_patches):\n",
    "\n",
    "    X_train = np.array(th.squeeze(X_train)); Y_train = np.array(Y_train)\n",
    "    X_valid = np.array(th.squeeze(X_valid)); Y_valid = np.array(Y_valid)\n",
    "\n",
    "    L_seq = X_train.shape[1]\n",
    "    L_sub_patch = int(L_seq / N_sub_patches)\n",
    "    N_features = 10 # mean, std, skew, kurt, min, max, quantiles 0.25, 0.5, 0.75, inter-quartile range\n",
    "  \n",
    "    def rearranging_X(X):\n",
    "        \n",
    "        X_rearranged = np.zeros((X.shape[0], N_sub_patches, N_features))\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(N_sub_patches):\n",
    "                xj = X[i,j*L_sub_patch:(j+1)*L_sub_patch]\n",
    "                X_rearranged[i,j,:] = [xj.mean(), xj.std(), np.double(scipy.stats.skew(xj)), \n",
    "                                       np.double(scipy.stats.kurtosis(xj)), xj.min(), xj.max(), \n",
    "                                       np.quantile(xj, 0.25), np.quantile(xj, 0.5), np.quantile(xj, 0.75), \n",
    "                                       np.quantile(xj, 0.75) - np.quantile(xj, 0.25)]\n",
    "        return X_rearranged\n",
    "\n",
    "    X_train_rearranged = rearranging_X(X_train); X_valid_rearranged = rearranging_X(X_valid)\n",
    "\n",
    "    X_train = th.from_numpy(X_train_rearranged); X_valid = th.from_numpy(X_valid_rearranged)\n",
    "    Y_train = th.from_numpy(Y_train); Y_valid = th.from_numpy(Y_valid)\n",
    "    Y_train = th.unsqueeze(Y_train, -1); Y_valid = th.unsqueeze(Y_valid, -1)\n",
    "\n",
    "    return X_train, Y_train, X_valid, Y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CFL8PvDnjZvs"
   },
   "outputs": [],
   "source": [
    "X_train_, Y_train_, X_valid_, Y_valid_ = data_preparation_2(X_train, Y_train, X_valid, Y_valid, size_patch, N_sub_patches = N_sub_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VgH9isMrwZkp"
   },
   "outputs": [],
   "source": [
    "#Inspired from https://cnvrg.io/pytorch-lstm/\n",
    "\n",
    "class LSTM1(nn.Module):\n",
    "    def __init__(self, N_features, hidden_size, num_layers, seq_length):\n",
    "        super(LSTM1, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.N_features = N_features # Number of features\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "        self.seq_length = seq_length #sequence length\n",
    "\n",
    "        self.lstm = nn.LSTM(N_features=N_features, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True) #lstm => Input Shape : (N_batch, L_seq, N_feature)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        h_0 = th.zeros(self.num_layers, x.size(0), self.hidden_size) #hidden state\n",
    "        c_0 = th.zeros(self.num_layers, x.size(0), self.hidden_size) #internal state\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x.float(), (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "        out = hn\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gc9D9u4T1C33"
   },
   "outputs": [],
   "source": [
    "#num_classes = 1 #number of output classes => irrelevant here because we are not performing classification\n",
    "#if user_in == 1:\n",
    "#    L_seq = X_train_.shape[2]\n",
    "#else:\n",
    "#    L_seq = X_train_.shape[1] # length of the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g7DE7GjCGeRR"
   },
   "outputs": [],
   "source": [
    "model = LSTM1(N_features, hidden_size, num_layers, L_seq) #our lstm class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2K4Yrlj4GsNw",
    "outputId": "7a404d0a-dd17-4331-8dab-61f14a344094"
   },
   "outputs": [],
   "source": [
    "output = model(th.squeeze(X_train_[0]))\n",
    "\n",
    "print(\"output1 = {}; \\noutput2 = {}\".format(output[0,0], output[1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9m0tjrtEzO2"
   },
   "outputs": [],
   "source": [
    "criterion = th.nn.MSELoss()    # mean-squared error for regression\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBh1-BEJ3-AA"
   },
   "outputs": [],
   "source": [
    "# Problème avec l'entraînement du LSTM => C'est hyper long avec des séries de taille 1500000 !!!\n",
    "# Plusieurs stratégies:\n",
    "# - Soit on change le format des données (on fait des coupes de longueur 250-500, préconisées pour les LTSM)\n",
    "#   risque: grosse perte d'infos, temps trop court pour être informatif. En plus, ne répond pas vraiment au cahier des charges\n",
    "# - Soit on garde les mêmes séquences, mais on les sous-découpe en patchs sur lesquelles on calcule la variance, la moyenne, la kurtosis.. etc => pleins de paramètres \n",
    "#   statistiques et on fabrique une séquence avec autant de paramètres, aussi longue que le nombre de patchs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1k1geqipE79l",
    "outputId": "67aa1abe-5aee-49a5-84f3-7d627ed8c6cc"
   },
   "outputs": [],
   "source": [
    "# Training LSTM\n",
    "\n",
    "valid_losses = np.zeros((num_epochs))\n",
    "train_losses = np.zeros((num_epochs))\n",
    "\n",
    "min_vl = 1000\n",
    "\n",
    "best_mvd = []\n",
    "best_mtd = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model.forward(X_train_) #forward pass\n",
    "    optimizer.zero_grad() #caluclate the gradient, manually setting to 0\n",
    "    # obtain the loss function\n",
    "    loss = criterion(outputs, Y_train_)\n",
    " \n",
    "    loss.backward() #calculates the loss of the loss function\n",
    "\n",
    "    train_loss = loss.item()\n",
    "    train_losses[epoch] = train_loss\n",
    "\n",
    "    with th.no_grad():\n",
    "        \n",
    "        valid_output = model(X_valid_)\n",
    "        valid_loss = criterion(valid_output, Y_valid_)\n",
    "        valid_losses[epoch] = valid_loss.item()\n",
    "        valid_differences = valid_output - Y_valid_\n",
    "\n",
    "        #valid_loss, valid_differences = check_accuracy(X_valid, Y_valid, model)\n",
    "\n",
    "        if valid_loss < min_vl:\n",
    "\n",
    "            min_vl = valid_loss\n",
    "            min_tl = train_loss\n",
    "\n",
    "            best_mvd = valid_differences\n",
    "\n",
    "            Y_final = model(X_train_)\n",
    "\n",
    "            best_mtd = Y_train_ - Y_final\n",
    "            best_mtd = best_mtd.numpy()\n",
    "            #best_mtd = best_mtd.cpu().detach().numpy()\n",
    "        \n",
    " \n",
    "    optimizer.step() #improve from loss, i.e backprop\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHuZqNl2LWTl",
    "outputId": "2ee50015-0dd0-4cfa-ba2d-c16fd7bafbca"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(10, 5))\n",
    "fig.subplots_adjust(hspace=0.4, wspace = 0.4)\n",
    "ax[0,0].plot(train_losses, 'r', label = \"training\")\n",
    "ax[0,1].plot(valid_losses, 'b', label = \"validation\")\n",
    "ax[1,0].plot(train_losses, 'r', label = \"training\")\n",
    "ax[1,0].plot(valid_losses, 'b', label = \"validation\")\n",
    "ax[1,0].set(title = \"Losses\", xlabel = \"epochs\", ylabel = \"MSE\")\n",
    "test = np.array(Y_valid)\n",
    "ax[1,1].plot(test, 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'LSTM_004'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_and_save_results(train_losses, valid_losses, best_mvd, best_mtd, mm, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = pickle.load(open(model_name + \"display.p\", \"rb\" ))\n",
    "print(model_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:earthquake-prediction]",
   "language": "python",
   "name": "conda-env-earthquake-prediction-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
